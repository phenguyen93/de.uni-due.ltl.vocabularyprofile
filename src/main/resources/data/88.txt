Writing skills are essential for success in school, career, and society. Unfortunately, students in the
United States are not performing well in writing. For example, on the most recent National Assessment
of Educational Progress (NAEP) writing assessment, nearly 75% of 8th and 11th grade students were
Basic or Below Basic writers (National Center for Education Statistics, 2012). This general lack of writing
proficiency carries over to the workplace, where it costs employers as much as $3.1 billion annually to
remedy writing deficiencies (National Commission on Writing, 2004).
Like learning any complex skill, writing development is largely a function of practice. However, teachers
juggle myriad instructional responsibilities and often have to limit students’ writing opportunities to
make grading manageable (Kellogg & Whiteford, 2009). Without extensive practice and regular
feedback, students’ writing performance will not substantively improve.
As computer‐based summative assessment becomes ubiquitous, so does the opportunity to report
assessment results in hours or days rather than weeks or months. However, performance tasks have
historically been scored by professional raters or teachers, a time‐consuming and costly process. Why
not omit performance tasks from summative assessments? As opposed to machine‐scored selected‐
response items, essays and other performance assessments tend to be more authentic, are more
appropriate for measuring deeper learning outcomes, and positively impact teaching and learning
(Darling‐Hammond & Adamson, 2010, 2013).
Automated scoring solutions have been applied successfully to address all of these issues.  
How it Works  
Automated‐scoring engines do not “read” student responses and assign scores in the way that people
do. Rather, they match various characteristics of responses with the scores assigned by expert raters
and use these relations to predict scores for new responses. Automated engines can score a variety of
response types, ranging from short answers of a few words or sentences to extended essays.  
Automated‐scoring engines often use machine learning to determine how features of responses relate
to scores assigned by expert raters. This is similar to the machine learning Netflix applies to subscribers’
viewing habit data and Amazon applies to customers’ page‐view and purchase data to make
recommendations. The main difference is that rather than predicting a collection of shows or products
best matched with the user, automated scoring‐engines predict the single, most appropriate score for
each response.